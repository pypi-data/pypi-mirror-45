# data source spec, currently supported ones include
# CSV, MS Excel, Custom files as well as
# MySQL, Oracle and MSSQL
[DataSource]
Type = file.csv


# Metadata file source files locations and file properties
[DsMetadataFile]
# Metadata file location
MetadataLocation = metadata_location
# Metadata file pattern
FilePattern = *.json
# Metadata processor
MetadataProcessor = 


# Metadata Zookeeper source files locations and file properties
[DsMetadataZookeeper]
# Zookeeper host location
ZkHost = localhost:2181/twiddle
# Zookeeper ACL credentials, optional
ZkUsername =
ZkPassword =
# Metadata znode pattern
FilePattern = *
# Metadata processor
MetadataProcessor = 


# CSV source files locations and file properties
[DsFileCsv]
SourceLocation = source_data
ArchiveLocation = archive_data
FailLocation = fail_data
FilePattern = *.csv
ColumnSeparator = ,
DecimalPoint = .
# Options based on pandas read_csv compression options
# Link: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html
Compression = infer


[DsFileJson]
SourceLocation = source_data
ArchiveLocation = archive_data
FailLocation = fail_data
FilePattern = *.json


# Excel source files locations and file properties
[DsFileExcel]
SourceLocation = source_data
ArchiveLocation = archive_data
FailLocation = fail_data
FilePattern = *.xlsx
# Default Sheets is empty for Sheets, use all sheets 
Sheets = 


# Custom source files locations and file properties
[DsFileCustom]
SourceLocation = source_data
ArchiveLocation = archive_data
FailLocation = fail_data
# Default FilePattern is empty, use all files
FilePattern = 
# There is no default for FileParser, this must be specified
FileParser = 


# MySQL connector 
[DsDatabaseMysql]
# Server host name incl port, must be specified
DbServer = 
# Database name to use, must be specified
DbName = 
# Database connection username and password, optional
DbUsername = 
DbPassword = 
# Database tables to use
TablePattern = *
# Database table columns to use, default is empty to use all columns
TableColumns = 
# Watermark specs are necessary only for incremental processing.
WatermarkColumn = 
WatermarkStore = high_watermarks.db
ResetWatermark = True


# Oracle connector 
[DsDatabaseOracle]
# Server host name incl port, must be specified
DbServer = 
# Database name to use, must be specified
DbName = 
# Database connection username and password, optional
DbUsername = 
DbPassword = 
# Database tables to use
TablePattern = *
# Database table columns to use, default is empty to use all columns
TableColumns = 
# Watermark specs are necessary only for incremental processing.
WatermarkColumn = 
WatermarkStore = high_watermarks.db
ResetWatermark = True


# MsSQL connector 
[DsDatabaseMssql]
# Server host name incl port, must be specified
DbServer = 
# Database name to use, must be specified
DbName = 
# Database connection username and password, optional
DbUsername = 
DbPassword = 
# Database tables to use
TablePattern = *
# Database table columns to use, default is empty to use all columns
TableColumns = 
# Watermark specs are necessary only for incremental processing.
WatermarkColumn = 
WatermarkStore = high_watermarks.db
ResetWatermark = True


[DsDatabaseSqlite]
# Db Path, no default
DbPath = 
# Database tables to use, default is every table 
TablePattern = 
# Database table columns to use, default is empty to use all columns
TableColumns = 
# Watermark specs are necessary only for incremental processing.
WatermarkColumn = 
WatermarkStore = high_watermarks.db
ResetWatermark = True


[DsMongo]
# Database host, default "localhost"
MongoServer = localhost
MongoPort = 27017
MongoUsername = 
MongoPassword = 
MongoQuery = { }


# Specification of mapping source data columns to repository data columns
# Mapper is specified in a csv file
[Mapper]
File = mapper/mapper.csv
# Column type for the mapper file itself.
# Optional, useful if value type are ambiguous
ColumnType = {"ignore":"str", "allow_missing":"str"}
# Rows in the mapper file to use, specified via column dataset
# Default is empty, use all rows
DataSets = 


# Data Repository spec.
# Repository is where the processed data is sent
[DataRepository]
Type = solr


# Solr repository spec
[RepositorySolr]
# Zookeeper host location, for SolrCloud.
# Overridden by SolrUrl (if SolrUrl is specified, ZkHost is ignored)
ZkHost = localhost:2181/solr
# Zookeeper ACL credentials, optional
ZkUsername =
ZkPassword =
# Solr server url for connecting to Solr directly
SolrUrl = http://localhost:8983/solr
# Values for SolrSslVerify are True/False or SSL certificate filepath
SolrSslVerify = True
SolrCollection = Test
SolrUsername = admin
SolrPassword = password


# If repository schema will be (re)built, default is true
BuildSchema = True
# File containing additional solr field types
# Default is empty, meaning no additional field types
UserTypeFile = 
# Extra solr fields in addition to those specified in the mapper file
ExtraFields = {"filename" : "string", "id": "string"}
# If StrictSchema is True, the type of a solr field cannot be changed
StrictSchema = False
# Number of rows/documents sent to Solr per chunk
ChunkSize = 500
# Should zero value fields be removed from a row/document
RemoveZeroValues = True


# CSV repository spec
[RepositoryCsv]
# output file, required
FilePath = 
ColumnSeparator = ,
DecimalPoint = .
Append = False


[Logging]
Name = Twiddle
Format = '%%(levelname)s %%(name)s %%(message)s'
Level = DEBUG


# Processing specification, all the specified functions are in 
# the user provided file, local_functions.py
[Processing]
# If true, the app will pause and wait for more data after processing
WaitForData = False
# PreMapTransformationProd is a function that massages dataframe
# before any validation is performed on the data.
PreMapTransformationProc =
# SourceColumnHeaderTidier is a function that massages dataframe 
# column headers before any processing to, e.g. take care of mixed case 
# column headers. 
# Functions defined in utils.py, that is packaged with the app, can also 
# be used, as well as those in local_functions.py
# Function takes one single parameter of str type and returns a str
#Default is empty, i.e not tidying.
SourceColumnHeaderTidier = 
# Function to transform data frame 
# take one parameter of dataframe type and returns a dataframe
#Default is empty, i.e not transformation.
TransformationProc = transform_proc
# Function to perform cross sheet data manipulation
# take one parameter of type dict of dataframes and returns a dict of dataframes
#Default is empty, i.e not transformation.
ExcelCrossSheetProc = cross_sheet_proc
