####
## Output descriptions:
##

# Treasure Data (http://www.treasure-data.com/) provides cloud based data
# analytics platform, which easily stores and processes data from td-agent.
# FREE plan is also provided.
# @see http://docs.fluentd.org/articles/http-to-td
#
# This section matches events whose tag is td.DATABASE.TABLE
#<match td.*.*>
#  @type tdlog
#  @id output_td
#  apikey YOUR_API_KEY
#
#  auto_create_table
#  <buffer>
#    @type file
#    path /var/log/td-agent/buffer/td
#  </buffer>
#
#  <secondary>
#    @type file
#    path /var/log/td-agent/failed_records
#  </secondary>
#</match>

## match tag=debug.** and dump to console
#<match debug.**>
#  @type stdout
#  @id output_stdout
#</match>

####
## Source descriptions:
##

## built-in TCP input
## @see http://docs.fluentd.org/articles/in_forward
#<source>
#  @type forward
#  @id input_forward
#</source>

## built-in UNIX socket input
#<source>
#  type unix
#</source>

# HTTP input
# POST http://localhost:8888/<tag>?json=<json>
# POST http://localhost:8888/td.myapp.login?json={"user"%3A"me"}
# @see http://docs.fluentd.org/articles/in_http
#<source>
#  @type http
#  @id input_http
#  port 8888
#</source>

## live debugging agent
#<source>
#  @type debug_agent
#  @id input_debug_agent
#  bind 127.0.0.1
#  port 24230
#</source>

####
## Examples:
##

## File input
## read apache logs continuously and tags td.apache.access
#<source>
#  @type tail
#  @id input_tail
#  <parse>
#    @type apache2
#  </parse>
#  path /var/log/httpd-access.log
#  tag td.apache.access
#</source>

## File output
## match tag=local.** and write to file
#<match local.**>
#  @type file
#  @id output_file
#  path /var/log/td-agent/access
#</match>

## Forwarding
## match tag=system.** and forward to another td-agent server
#<match system.**>
#  @type forward
#  @id output_system_forward
#
#  <server>
#    host 192.168.0.11
#  </server>
#  # secondary host is optional
#  <secondary>
#    <server>
#      host 192.168.0.12
#    </server>
#  </secondary>
#</match>

## Multiple output
## match tag=td.*.* and output to Treasure Data AND file
#<match td.*.*>
#  @type copy
#  @id output_copy
#  <store>
#    @type tdlog
#    apikey API_KEY
#    auto_create_table
#    <buffer>
#      @type file
#      path /var/log/td-agent/buffer/td
#    </buffer>
#  </store>
#  <store>
#    @type file
#    path /var/log/td-agent/td-%Y-%m-%d/%H.log
#  </store>
#</match>

<system>
  log_level error
</system>

## read iserver logs and send to es

# parse iost.log
<source>
  @type tail
  @id input_tail
  path {{ iserver.logfile }}
  pos_file /var/log/td-agent/iost.log.pos
  #format /^(?<lvl>\w+) (?<time>[\d-]+ [\d:]+\.\d+) (?<file>[\w.]+):(?<line>\d+) (?<msg>.*)$/
  #time_format %Y-%m-%d %H:%M:%S.%L
  #keep_time_key true
  format multiline
  format_firstline /^(?<lvl>\w+) (?<date>[\d-]+) (?<timing>[\d:]+\.\d+) (?<file>[\w.]+):(?<line>\d+) /
  format1 /^(?<lvl>\w+) (?<date>[\d-]+) (?<timing>[\d:]+\.\d+) (?<file>[\w.]+):(?<line>\d+) (?<msg>.*)/
  types line:integer
  tag iost.log
</source>

# add fields
<filter iost.log>
  @type record_transformer
  <record>
    cluster {{ cluster_name }}
    node {{ inventory_hostname }}
    time ${record["date"]}T${record["timing"]}
  </record>
  remove_keys date,timing
</filter>

# send to es
<match iost.log>
  @type elasticsearch
  host {{ elasticsearch.host | d('localhost') }}
  port {{ elasticsearch.port | d(80) }}
  user {{ elasticsearch.user | d('user') }}
  password {{ elasticsearch.pass | d('pass') }}
  index_name {{ cluster_name }}-%Y.%m.%d
  <buffer tag, cluster, time>
    timekey         5m # chunks every 300s
    timekey_wait    10 # default: 10m
    flush_mode interval
    retry_type exponential_backoff
    flush_thread_count 2
    flush_interval 30s
    retry_max_interval 30
    retry_max_times 20
    chunk_limit_size 2M
    queue_limit_length 128
    overflow_action block
  </buffer>
</match>
